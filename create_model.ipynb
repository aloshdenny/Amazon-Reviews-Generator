{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipfile\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "from itertools import compress\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
    "                         AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         TrainingArguments, BeamScorer, Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
    "                             RandomSampler, SequentialSampler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG           = False\n",
    "\n",
    "INPUT_DIR       = 'articles'\n",
    "\n",
    "USE_APEX        = True\n",
    "APEX_OPT_LEVEL  = 'O1'\n",
    "\n",
    "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
    "\n",
    "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
    "\n",
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",                    \n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "                    \n",
    "MAXLEN          = 256  #{768, 1024, 1280, 1600}\n",
    "\n",
    "TRAIN_SIZE      = 0.8\n",
    "\n",
    "if USE_APEX:\n",
    "    TRAIN_BATCHSIZE = 16\n",
    "    BATCH_UPDATE    = 128\n",
    "else:\n",
    "    TRAIN_BATCHSIZE = 8\n",
    "    BATCH_UPDATE    = 256\n",
    "\n",
    "EPOCHS          = 3\n",
    "LR              = 5e-4\n",
    "EPS             = 1e-8\n",
    "WARMUP_STEPS    = 1e2\n",
    "\n",
    "SEED            = 2020\n",
    "\n",
    "\n",
    "DEVIDE_BY = 16\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('D:/amazon_1gb/train.csv',header=None)\n",
    "test_df = pd.read_csv('D:/amazon_1gb/test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "train_df = train_df.astype('str')\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.244\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "sample_num = 1000\n",
    "for review in train_df.sample(sample_num).iloc[:, 2]:\n",
    "    sum += len(review.split(' '))\n",
    "print(sum/sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 224,995 samples for training, and 4,999 samples for validation testing'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For debug\n",
    "train_df = train_df.sample(int(len(train_df) / DEVIDE_BY))\n",
    "test_df = test_df.sample(int(len(test_df) / DEVIDE_BY / 5))\n",
    "f'There are {len(train_df) :,} samples for training, and {len(test_df) :,} samples for validation testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, randomize=True):\n",
    "        self.randomize = randomize\n",
    "        self.tokenizer = tokenizer \n",
    "        self.title     = data.iloc[:, 1].tolist()\n",
    "        self.text      = data.iloc[:, 2].tolist()\n",
    "\n",
    "\n",
    "    #---------------------------------------------#\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    #---------------------------------------------#\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        input = SPECIAL_TOKENS['bos_token'] + self.title[i] + SPECIAL_TOKENS['sep_token'] + self.text[i] + SPECIAL_TOKENS['eos_token']\n",
    "\n",
    "        encodings_dict = tokenizer(input,                                   \n",
    "                                   truncation=True, \n",
    "                                   max_length=MAXLEN, \n",
    "                                   padding=\"max_length\")   \n",
    "        \n",
    "        input_ids = encodings_dict['input_ids']\n",
    "        attention_mask = encodings_dict['attention_mask']\n",
    "        \n",
    "        return {'label': torch.tensor(input_ids),\n",
    "                'input_ids': torch.tensor(input_ids), \n",
    "                'attention_mask': torch.tensor(attention_mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, S=TRAIN_SIZE):\n",
    "    train_data = data.sample(frac = TRAIN_SIZE)\n",
    "    val_data = data.drop(train_data.index)\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(special_tokens=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
    "\n",
    "    # Set eos_token as the padding token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        print(\"Special tokens added\")\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
    "\n",
    "    #GPT2LMHeadModel\n",
    "    if special_tokens:\n",
    "        config = AutoConfig.from_pretrained(MODEL, \n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            sep_token_id=tokenizer.sep_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            output_hidden_states=False)\n",
    "    else: \n",
    "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            output_hidden_states=False)    \n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config, from_tf=True)\n",
    "    if special_tokens:\n",
    "        #Special tokens added, model needs to be resized accordingly\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if load_model_path:\n",
    "        model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "    model.cuda()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.98 s\n",
      "Wall time: 4.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = get_tokenizer(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                #   load_model_path='pytorch_model.bin'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Freeze selective layers:\n",
    "# - Freeze all layers except last n:\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "for i, m in enumerate(model.transformer.h):        \n",
    "    #Only un-freeze the last n transformer blocks\n",
    "    if i+1 > 12 - UNFREEZE_LAST_N:\n",
    "        for parameter in m.parameters():\n",
    "            parameter.requires_grad = True \n",
    "\n",
    "for parameter in model.transformer.ln_f.parameters():        \n",
    "    parameter.requires_grad = True\n",
    "\n",
    "for parameter in model.lm_head.parameters():        \n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = myDataset(train_df, tokenizer)\n",
    "val_dataset = myDataset(test_df, tokenizer, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "c:\\Users\\alosh\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9314768df9f945ebbbf2c76efe0ce13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1755 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2619, 'learning_rate': 0.00037915407854984895, 'epoch': 0.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8133d3406f45c3bb60926497ba8490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.346152424812317, 'eval_runtime': 83.9003, 'eval_samples_per_second': 59.583, 'eval_steps_per_second': 19.869, 'epoch': 1.0}\n",
      "{'loss': 1.388, 'learning_rate': 0.0002280966767371601, 'epoch': 1.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6046a937cf474651a680f2d744249e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.330527901649475, 'eval_runtime': 83.9503, 'eval_samples_per_second': 59.547, 'eval_steps_per_second': 19.857, 'epoch': 2.0}\n",
      "{'loss': 1.359, 'learning_rate': 7.70392749244713e-05, 'epoch': 2.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4a2923bff641c28a662f9762e40660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.325377345085144, 'eval_runtime': 83.2588, 'eval_samples_per_second': 60.042, 'eval_steps_per_second': 20.022, 'epoch': 3.0}\n",
      "{'train_runtime': 48920.2208, 'train_samples_per_second': 13.798, 'train_steps_per_second': 0.036, 'train_loss': 1.6227318744713763, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "#UNCOMMENT THESE TO TRAIN THE GPT MODEL (ETA: 7 HRS)\n",
    "#%%time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=BATCH_UPDATE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    fp16=True,\n",
    "    fp16_opt_level=APEX_OPT_LEVEL,\n",
    "    warmup_steps=WARMUP_STEPS,    \n",
    "    learning_rate=LR,\n",
    "    adam_epsilon=EPS,\n",
    "    weight_decay=0.01,        \n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to = None,\n",
    ")\n",
    "\n",
    "#---------------------------------------------------#\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,    \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#---------------------------------------------------#\n",
    "trainer.train()\n",
    "trainer.save_model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                  load_model_path='pytorch_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Wings of Fire\"\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + title + SPECIAL_TOKENS['sep_token'] \n",
    "         \n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "device = torch.device(\"cuda\")\n",
    "generated = generated.to(device)\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: of Fire<|SEP|>Lance(S)^[C]{3}H*EJX'I,6O\\R9KQ5Y-F2G4ZU?V8D.A7N0@M!T+#$1 M=&_`;%',/:~\"^^'/\",/- '?''; - I'-':,,/,,-:- \"'\".:' (i-'.'..''...), ;■• ■£.-/. :,-. i'.-,j:, /,. a.: e.* Jn,'.; >-- = ou *., ua 1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100\n",
      "—–»« » « ——— | ^r r nf\n",
      "\n",
      "\n",
      "2: of Fire<|SEP|>Lords Of War</b><i>\"War\"/\"~^[a]$'(.&=\\@+#%/S,0T;?4I_9Z1X5V6Y3C-J2MQ8KG)7A`E*RHUO!D}F:NKK MIXED \"** WATER](\\\\.*)/\\/\n",
      "\n",
      "\n",
      "\n",
      "3: of Fire<|SEP|>Achievements[0-19]\n",
      " (1) \"Fire\" Level 1, Skill Points: 100% | [100%] Lv. 4 - 9 XP Required to level up the skill in any game mode or by using a special ability that grants you additional points from skills and abilities for your own purposes! <~=The first time I ever played with it on my 3DS was at E3 2014 when we were playing Super Mario 64's Wii U version...I didn't know how long this would take me before people started buying more games so there is no way back now.. =(\n",
      "\n",
      "…the fact remains though they are still available through their website.) This will be fixed soon as well since these things aren´t really needed anymore because after all those years without them i can just buy something new every day :)\n",
      "\n",
      "\n",
      "4: of Fire<|SEP|>Darkspawn</a> (2) <b><br />The Dark Brotherhood is a group that has emerged from the ashes and continues to exist in an ancient, peaceful world. They are known as 'the Shadow Brokers', because they have created new ones within their ranks... but also those who once existed inside them.\" -Alfie Smith[src]\n",
      "\"When you enter into this realm your power will become yours,\" said Alduin when he spoke with Alesha after she had left her home.[1][3]:19-21 The Black Tower was founded by Brienne Baratheon's sons upon his arrival at Dorne whereupon it became one place for him during battle against Lordaerys Targaryen on Vvardenfell. He then returned there before returning again shortly thereafter,[4]. During such time period both Houses were able successfully rebuild themselves under King Daeneris' leadership through trade between all three parties: Sanso Stark, Jon Snow and House Martell Lannister; however some factions believed Maester Hightower should remain loyalist despite being exiled due not only its political position towards Westeros politics [5], which led to tensions among\n",
      "\n",
      "\n",
      "5: of Fire<|SEP|>Locations <|CIT-|>Stones</= |HUMBLES]\\ \\ (0) [2:00.03 - 2:[1]: 0, 1 : 6\n",
      "[3:-10-11:] -> The following lines are the same as above but do not match with any other text in this file and should be replaced by an empty line after each character or two.[^5]-8-[4:]-> This is a newline which must appear before every word that follows it unless explicitly set to \"\". It will never occur within quotes except on those spaces where there has been no preceding space; all quotations from here shall remain at their original position for future reference when they may reappear again.\"[*+7\\-][9]) ^6-,(#-) = +/-/ /([~]]$/,@:/_?\\\\'%/.,/(?).-. * '',\\.('). '. ;.(.).....`.'..';.;! `.-'. ('.]`. ~-'!'./:'.,.?\\''/=/.*?''-':!.''\"'%.'>'\".:\"\"....:\",'--''.!...,.{......}\n",
      "\n",
      "\n",
      "6: of Fire<|SEP|>Drones. <-- I don't think the only way to do this is by getting some kind OF SOUND METHOD, but if you're not a computer programmer then it's probably better that they use something like R2Q or Gigaom than anything else...\n",
      "posted 11:46 AM @davidkrausch The idea behind using these sounds as sound effects on your games and making them appear in other peoples' environments was first proposed back when he came up with his own game Sound Effects for Minecraft. It turns out all those little \"sound\" files are actually quite useful stuff! So let me share what we've been able get so far - here comes my second project (and one which would make sense without any additional code):The music effect used within each level can be found at : https://www-gameplaygamesguide/en_US/?hl=EN&hls%20FDE%. You'll need an HTML5 capable browser such AsyncPlayerWebGL 2+ Chrome Edge Firefox Internet Explorer Opera Safari Android webview Samsung Gear VR Rift DK1 Mobile Facebook Twitter Google Whatsapp Pinterest Print Email Version 1\n",
      "\n",
      "\n",
      "7: of Fire<|SEP|>The End Of The World(Fairytale) <0:00 - 0.05% | 100%: 25%, 50%.\n",
      "\"I've never had a chance to play this game before and I'm very pleased with how it plays.\" \"It's not as bad if you want something that looks like real life, but the characters in some places feel much more human than they do on screen,\" said Koyama-sensei who has played many games from his childhood playing chess or basketball at school (he was also an avid gamer).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8: of Fire<|SEP|>Drones</a></b><br/> <div class=\"g\">[[^[1]]](http://www.redditnews and /r/IAmA)#!/comments/5zqk6s0,15678716,-2,,<<if $motivation eq 4>>You know what's funny? You're going to die when you don't have a girlfriend.</i>\" }, \"$:/core\": { \"title\":\"Misc\", \"(text)\", \"/*\\\\\n",
      "\"matchbox=\\\"$:/language/$(\\\"\\\")+/(?:show-temp or!todo=null))*///{{{ }}}\n",
      "\n",
      ":([ \\w ]+)[/v] */ ///////////////////////////////// {{Title text: The game will be played from the main menu after every 5 minutes if it is not already playing (in this case in 2 hours). This means that all users are allowed access on your console for only 3 games per day - at least once each month! For example I would suggest 1 person play one match during 7 days with no friends left until midnight Pacific Time as my schedule doesn`T allow me enough time between matches so please use those times\n",
      "\n",
      "\n",
      "9: of Fire<|SEP|>Crimson Blade|Sorcery</b></h2><br /> <div class=\"c3-p1\"><a href=\"#\" target='_blank' title = \"\"></A>> </span>\" \"<ul id=\"> {{this.href}} -<<nobr>>\\ <<set $favoritedName to 'Hateful'>Gates Of War (10)</u$button[5].flip();}}} @@lend #targets {fontsize:40px;padding:-30px;}.listingList li a{color:#000} h4, td,.sides..., span!important ; /* * */ +/* *************************\\ | HIDE THE MISSION OF TERRORISM \\ *****************************************************************************/ p(document).onclick={function(){ var ctxInnerTicketEventArgs=[]; for($i in 0.. 2){ if ($j [0] === null &&!isset('id') || isspace(-7)) throw new Exception(\"Invalid ticket\"); return false } function checkTickets() {} /** Create an entry point with the following parameters that will allow you access into this mission and all\n",
      "\n",
      "\n",
      "10: of Fire<|SEP|>Guns</font></p><o:S1_F2L6n4] ==========[-#version 1.0][*]-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-+-----+-----------==---===----- | HP/MP - Magic / Attack : 50% +10 MP ---------- [Ascension (Curse)], Ethereal Armor, Berserk <|ROCKER](https://i3!Qj7qwEtYdTZk5yHfMzI9mXuNg8sJbVxO&})|Magic Item Boosting Potion to Level 20 and up in the shop at level 5 or above with your normal magic item(if you have it already), use that potion after reaching 6th tier as soon afterwards.[^Ulfaloo~!] ||===================>||=====[DARKNESS]]-|================\\================================================================================================____/_=/[[Darkness],[ADAMAGE/(20)]-(@%)\\\\([Evil])$((?:\\.(?:))?))(.*)/(\\`)\\\\.\\/' \\\\\\\\\\]+\\\" ';//\\/\\/[]*/<<@@>> <<+= \"./\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top-p (nucleus) text generation (10 samples):\n",
    "sample_outputs = model.generate(generated, \n",
    "                                do_sample=True,   \n",
    "                                min_length=50, \n",
    "                                max_length=MAXLEN,\n",
    "                                top_k=30,                                 \n",
    "                                top_p=0.7,        \n",
    "                                temperature=0.9,\n",
    "                                repetition_penalty=2.0,\n",
    "                                num_return_sequences=10\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    a = len(title)  \n",
    "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: of Fire<|SEP|>Razor-Wraith</b><br /> <a href=\"http://www.netflix.com/browse/genre/1,539\" rel=\"nofollow\">Fantastic Beasts and Where to Find Them</a></p><p align='Left'><font face='Strat2Medium' color='#FFFFFF' size='14'></font></p><p align='Left'><font face='Strat2Medium' color='#CCCCCC' size='14'>Locked<br>Next Rank Cost: 1 points</font></p> STRIPPED TEXT = FANTASTIC BEINGS AND WHERE TO Find Them Locked Next Rank Cost: 1 points RANKUP HTML = <p align='Left'><font face='Strat2Medium' color='#CCCCCC' size='14'>Next Rank Cost: 1 points</font></p><p align='Left'><font face='Strat2Medium' color='#28AA00' size='14'>+10% Base Damage Rating when you take damage from a Critical Strike.</font></p> RANKUP STRIPPED TEXT = Next\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beam-search text generation:\n",
    "sample_outputs = model.generate(generated, \n",
    "                                do_sample=True,   \n",
    "                                max_length=MAXLEN,                                                      \n",
    "                                num_beams=5,\n",
    "                                repetition_penalty=5.0,\n",
    "                                early_stopping=True,      \n",
    "                                num_return_sequences=1\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    a = len(title) \n",
    "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Generating raw text with GPT-2\n",
    "tokenizer = get_tokenizer()\n",
    "model = get_model(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Wings of Fire\n",
      "\n",
      "In addition to being a great way to get your feet wet, it can also help reduce the amount of time you'll have to clean up after yourself. Here are some tips on how to keep your hands dry:\n",
      "\n",
      "Clean Your Hands After Cleaning Up\n",
      "\n",
      "Don't just wash your hands with soap and water. If you don't want to do that, try using an old toothbrush or scrubber instead. In fact, if you've ever washed your hands before, this is probably one of the best ways to make sure they're getting used to washing them properly.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = title\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "device = torch.device(\"cuda\")\n",
    "generated = generated.to(device)\n",
    "\n",
    "model.eval()\n",
    "sample_outputs = model.generate(generated, \n",
    "                                do_sample=True,   \n",
    "                                max_length=MAXLEN,                                                      \n",
    "                                num_beams=5,\n",
    "                                repetition_penalty=5.0,\n",
    "                                early_stopping=True,      \n",
    "                                num_return_sequences=1\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
